{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# L3: Measuring Cache Effectiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you‚Äôll learn how to evaluate your cache using metrics like hit rate, precision, recall, and latency to understand its real impact.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ <b>Note <code>(Kernel Starting)</code>:</b> This notebook takes about 30 seconds to be ready to use. You may start and watch the video while you wait.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 268
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from cache.evals import CacheEvaluator\n",
    "from cache.faq_data_container import FAQDataContainer\n",
    "from cache.wrapper import SemanticCacheWrapper\n",
    "from cache.config import config, load_openai_key\n",
    "\n",
    "print(\"üì¶ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> üíª &nbsp; <b>Access <code>requirements.txt</code> and <code>helper.py</code> files:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>.\n",
    "\n",
    "<p> ‚¨á &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Load Data and Recreate Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "data_container = FAQDataContainer()\n",
    "faq_df, test_df = data_container.faq_df, data_container.test_df\n",
    "\n",
    "test_queries = test_df[\"question\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Initialize semantic cache wrapper from config\n",
    "cache_wrapper = SemanticCacheWrapper.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Evaluating cache quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "# Cache hydration via wrapper helper\n",
    "cache_wrapper.hydrate_from_df(faq_df)\n",
    "\n",
    "cache_wrapper.check(faq_df[\"question\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "test_queries[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "cache_results = cache_wrapper.check_many(test_queries)\n",
    "cache_results[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "evaluator = CacheEvaluator(\n",
    "    true_labels=data_container.label_cache_hits(cache_results),\n",
    "    cache_results=cache_results,\n",
    ")\n",
    "evaluator.report_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "[[tn, fp], [fn, tp]] = evaluator.get_metrics()[\"confusion_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "tn[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "evaluator.matches_df()[fp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating cache latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "def simulate_llm_call(prompt):\n",
    "    time.sleep(np.random.uniform(0.2, 0.5))\n",
    "    return f\"LLM response to {prompt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "from cache.evals import PerfEval\n",
    "\n",
    "perf_eval = PerfEval()\n",
    "\n",
    "with perf_eval:\n",
    "    for query in tqdm(test_queries):\n",
    "        cache_wrapper.check(query)\n",
    "        perf_eval.tick(\"cache_hit\")\n",
    "        perf_eval.start()\n",
    "        simulate_llm_call(query)\n",
    "        perf_eval.tick(\"llm_call\")\n",
    "\n",
    "metrics = perf_eval.get_metrics(labels=[\"cache_hit\", \"llm_call\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "metrics[\"by_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "perf_eval.plot(\n",
    "    title=\"Performance Comparison\", show_cost_analysis=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"><b>Note:</b> In the above experiment we measure the latency of the cache response and a mocked latency of an LLM call. The mocked LLM call is a dummy function that sleeps for a random amount of time. The randomness in the results mainly comes from the randomness we introduced to mock the LLM. The results show us what we can typically see in practice.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "llm_latency = metrics[\"by_label\"][\"llm_call\"][\"average_latency\"]\n",
    "cache_latency = metrics[\"by_label\"][\"cache_hit\"][\"average_latency\"]\n",
    "\n",
    "cache_hit_rate = 0.3\n",
    "cached_llm_latency = llm_latency * (1 - cache_hit_rate) + cache_latency * cache_hit_rate\n",
    "cached_llm_drop_in_latency = (llm_latency - cached_llm_latency) / llm_latency\n",
    "cached_llm_speedup = llm_latency / cached_llm_latency\n",
    "print(f\"Overall latency drop of an LLM app: {int(cached_llm_drop_in_latency * 100)}%\")\n",
    "print(f\"Overall speedup of an LLM app {cached_llm_speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## LLM-as-a-Judge for cache quality evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> üö®\n",
    "&nbsp; <b>Different Run Results:</b> The output visualizations generated may differ from those shown in the video.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "cache_wrapper.hydrate_from_df(faq_df)\n",
    "\n",
    "# we set the distance to obtain even bad matches and evaluate if they are true negatives\n",
    "full_retrieval_nearest_neighbors = cache_wrapper.check_many(\n",
    "    test_queries, distance_threshold=1\n",
    ")\n",
    "full_retrieval_matches = [h.matches[0].prompt for h in full_retrieval_nearest_neighbors]\n",
    "full_retrieval_matches[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "load_openai_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from cache.llm_evaluator import LLMEvaluator\n",
    "\n",
    "evaluator = LLMEvaluator.construct_with_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "llm_similarity_results = evaluator.predict(\n",
    "    dataset=zip(test_queries, full_retrieval_matches),\n",
    "    batch_size=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "llm_similarity_results.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "# When evaluation is based on full retrieval we should use this constructor\n",
    "evaluator = CacheEvaluator.from_full_retrieval(\n",
    "    true_labels=llm_similarity_results.df[\"is_similar\"].values,\n",
    "    cache_results=cache_wrapper.check_many(test_queries),\n",
    ")\n",
    "evaluator.report_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "cache_wrapper.cache.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
