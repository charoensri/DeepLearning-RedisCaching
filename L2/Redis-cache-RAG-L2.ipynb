{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a964a4",
   "metadata": {},
   "source": [
    "# L2: Build Your First Semantic Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cda54ef-4d5b-4e9f-8e80-6d525b45e498",
   "metadata": {},
   "source": [
    "In this lab, you‚Äôll build a working semantic cache from scratch so you can see how each part works, and then you will implement it using Redis‚Äôs open source SDK and database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f24fbf5-3a9a-4b1c-a9b7-72fc2eb23cc0",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ <b>Note <code>(Kernel Starting)</code>:</b> This notebook takes about 30 seconds to be ready to use. You may start and watch the video while you wait.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed5e084-66ab-4514-9d5d-abfccd532534",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d1d5e-9eae-482b-951d-e61c4559ab67",
   "metadata": {},
   "source": [
    "## Load the FAQ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c4d0cf-12a3-4da3-957a-db3e8a8f20cc",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from cache.faq_data_container import FAQDataContainer\n",
    "\n",
    "faq_data = FAQDataContainer()\n",
    "faq_df = faq_data.faq_df\n",
    "test_df = faq_data.test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1bb291-4772-469d-ac16-c681b64e27bf",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> üíª &nbsp; <b>Access <code>requirements.txt</code> and <code>helper.py</code> files:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>.\n",
    "\n",
    "<p> ‚¨á &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667654ec-0f19-4141-b001-5c30efaf9df7",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "faq_df.head().style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embeddings-explanation",
   "metadata": {},
   "source": [
    "## Create Embeddings for Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06077b3",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "encoder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "faq_embeddings = encoder.encode(faq_df[\"question\"].tolist())\n",
    "\n",
    "print(f\"Sample (first 10 dimensions): {faq_embeddings[0][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7f48c7",
   "metadata": {},
   "source": [
    "## Implement Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc2ee07",
   "metadata": {
    "height": 336
   },
   "outputs": [],
   "source": [
    "def cosine_dist(a: np.array, b: np.array):\n",
    "    \"\"\"Compute cosine distance between two sets of vectors.\"\"\"\n",
    "    a_norm = np.linalg.norm(a, axis=1)\n",
    "    b_norm = np.linalg.norm(b) if b.ndim == 1 else np.linalg.norm(b, axis=1)\n",
    "    sim = np.dot(a, b) / (a_norm * b_norm)\n",
    "    return 1 - sim\n",
    "\n",
    "\n",
    "def semantic_search(query: str) -> tuple:\n",
    "    \"\"\"Find the most similar FAQ question to the query.\"\"\"\n",
    "    query_embedding = encoder.encode([query])[0]\n",
    "\n",
    "    distances = cosine_dist(faq_embeddings, query_embedding)\n",
    "\n",
    "    # Find the most similar question (lowest distance)\n",
    "    best_idx = int(np.argmin(distances))\n",
    "    best_distance = distances[best_idx]\n",
    "\n",
    "    return best_idx, best_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b23383d-92e5-4dcc-9a6c-9c1ec0a1f35f",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "idx, distance = semantic_search(\n",
    "    \"How long will it take to get a refund for my order?\"\n",
    ")\n",
    "\n",
    "print(f\"Most similar FAQ: {faq_df.iloc[idx]['question']}\")\n",
    "print(f\"Answer: {faq_df.iloc[idx]['answer']}\")\n",
    "print(f\"Cosine distance: {distance:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e077a3",
   "metadata": {},
   "source": [
    "## Build a Simple Semantic Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccbf7d4",
   "metadata": {
    "height": 268
   },
   "outputs": [],
   "source": [
    "def check_cache(query: str, distance_threshold: float = 0.3):\n",
    "    \"\"\"\n",
    "    Semantic cache lookup for previously asked questions.\n",
    "    Returns a dictionary with answer if hit, None if miss.\n",
    "    \"\"\"\n",
    "    idx, distance = semantic_search(query)\n",
    "\n",
    "    if distance <= distance_threshold:\n",
    "        return {\n",
    "            \"prompt\": faq_df.iloc[idx][\"question\"],\n",
    "            \"response\": faq_df.iloc[idx][\"answer\"],\n",
    "            \"vector_distance\": float(distance),\n",
    "        }\n",
    "\n",
    "    return None  # Cache miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db33021a-ca15-4434-b79a-6e3067fe4a75",
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    \"Is it possible to get a refund?\",\n",
    "    \"I want my money back\",\n",
    "    \"What are your business hours?\",  # Should miss\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    result = check_cache(query, distance_threshold=0.3)\n",
    "    if result:\n",
    "        print(f\"‚úÖ HIT: '{query}' -> {result['response'][:50]}...\")\n",
    "        print(f\"   Distance: {result['vector_distance']:.3f}\\n\")\n",
    "    else:\n",
    "        print(f\"‚ùå MISS: '{query}'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eff1ff7",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Add entries to the cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d125c948",
   "metadata": {
    "height": 302
   },
   "outputs": [],
   "source": [
    "def add_to_cache(question: str, answer: str):\n",
    "    \"\"\"\n",
    "    Add a new Q&A pair to our simple in-memory cache.\n",
    "    Extends both the DataFrame and embeddings matrix.\n",
    "    \"\"\"\n",
    "    global faq_df, faq_embeddings\n",
    "\n",
    "    new_row = pd.DataFrame({\"question\": [question], \"answer\": [answer]})\n",
    "    faq_df = pd.concat([faq_df, new_row], ignore_index=True)\n",
    "\n",
    "    # Generate embedding for the new question\n",
    "    new_embedding = encoder.encode([question])\n",
    "\n",
    "    # Add to embeddings matrix\n",
    "    faq_embeddings = np.vstack([faq_embeddings, new_embedding])\n",
    "\n",
    "    print(f\"‚úÖ Added to cache: '{question}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b65677-3bbe-4b59-800b-6c473a9fc4e4",
   "metadata": {
    "height": 370
   },
   "outputs": [],
   "source": [
    "print(\"Original cache size:\", len(faq_df))\n",
    "\n",
    "new_entries = [\n",
    "    (\n",
    "        \"What are your business hours?\",\n",
    "        \"We're open Monday-Friday 9 AM to 6 PM EST. Weekend support is available for urgent issues.\",\n",
    "    ),\n",
    "    (\n",
    "        \"Do you have a mobile app?\",\n",
    "        \"Yes! Our mobile app is available on both iOS and Android. Search for 'CustomerApp' in your app store.\",\n",
    "    ),\n",
    "    (\n",
    "        \"How do I update my payment method?\",\n",
    "        \"Go to Account Settings > Payment Methods to add, edit, or remove payment options.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "for question, answer in new_entries:\n",
    "    add_to_cache(question, answer)\n",
    "\n",
    "print(f\"\\nCache now has {len(faq_df)} total entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4468662-9d6e-44ce-bf28-205f762e4cff",
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "test_extended_queries = [\n",
    "    \"What time do you open?\",  \n",
    "    \"Is there a phone app?\", \n",
    "    \"How can I change my payment method?\",\n",
    "]\n",
    "\n",
    "for query in test_extended_queries:\n",
    "    result = check_cache(query, distance_threshold=0.3)\n",
    "    if result:\n",
    "        print(f\"‚úÖ HIT: '{query}' -> {result['response'][:50]}...\")\n",
    "        print(f\"   Distance: {result['vector_distance']:.3f}\\n\")\n",
    "    else:\n",
    "        print(f\"‚ùå MISS: '{query}'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "why-vector-database",
   "metadata": {},
   "source": [
    "## Moving to Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26476bd-d96f-4a25-a75a-c05185f7f81a",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e03eeee-34e8-4878-943a-abca510f193d",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "import redis\n",
    "\n",
    "try:\n",
    "    r = redis.Redis.from_url(REDIS_URL)\n",
    "    r.ping()\n",
    "    print(\"‚úÖ Redis is running and accessible\")\n",
    "except redis.ConnectionError:\n",
    "    print(\"‚ùå Cannot connect to Redis\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e02f7c",
   "metadata": {},
   "source": [
    "### Using a Cache-Optimized Embedding Model (langcache-embed-v1)\n",
    "https://huggingface.co/redis/langcache-embed-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10959d24",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "from redisvl.utils.vectorize import HFTextVectorizer\n",
    "from redisvl.extensions.cache.embeddings import EmbeddingsCache\n",
    "\n",
    "langcache_embed = HFTextVectorizer(\n",
    "    model=\"redis/langcache-embed-v1\",\n",
    "    cache=EmbeddingsCache(redis_client=r, ttl=3600)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00485b3",
   "metadata": {},
   "source": [
    "### Create the Redis Semantic Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-redisvl-cache",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "from redisvl.extensions.cache.llm import SemanticCache\n",
    "\n",
    "cache = SemanticCache(\n",
    "    name=\"faq-cache\",\n",
    "    vectorizer=langcache_embed,\n",
    "    redis_client=r,\n",
    "    distance_threshold=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a37aeb1",
   "metadata": {},
   "source": [
    "### Load the Cache with FAQ Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fee799",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "for i in range(len(faq_df)):\n",
    "    cache.store(\n",
    "        prompt=faq_df.iloc[i][\"question\"],\n",
    "        response=faq_df.iloc[i][\"answer\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4baa002-c799-48e1-950e-3d3d6c2128c1",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "result = cache.check(\"I need a refund for my purchase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02952a84-9fdf-47e4-85e2-c6af124c3d83",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cache-management",
   "metadata": {},
   "source": [
    "### Implement TTL (time-to-live) policy to keep cache fresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ee59fc",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "cache.set_ttl(86400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a525b4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## End-to-End LLM Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aec528-3312-4326-b791-3f26cffe0784",
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "from cache.config import load_openai_key\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "load_openai_key()\n",
    "\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.1,\n",
    "    max_tokens=150,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32b6a58-ed65-4e9f-8884-b6121ac724de",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "def get_llm_response(question: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful customer support assistant. Answer this customer question concisely and professionally:\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Provide a helpful response in 1-2 sentences. If you don't have specific information, give a general helpful response.\n",
    "    \"\"\"\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return response.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e359c0b5-bf67-4c82-b24e-109f3ff4b990",
   "metadata": {
    "height": 336
   },
   "outputs": [],
   "source": [
    "from cache.evals import PerfEval\n",
    "\n",
    "perf_eval = PerfEval()\n",
    "\n",
    "test_questions = [\n",
    "    \"How can I get my money back?\",\n",
    "    \"I want a refund please\",\n",
    "    \"What's your return policy?\",\n",
    "    \"I forgot my password\",\n",
    "    \"Can you help me reset my password?\",\n",
    "    \"What are your shipping costs?\",\n",
    "    \"Do you offer installation services?\",\n",
    "    \"Can I schedule a phone call with support?\",\n",
    "    \"How do I cancel my subscription?\",\n",
    "    \"How much does shipping cost?\",\n",
    "    \"I need to cancel my account\",\n",
    "]\n",
    "\n",
    "perf_eval.set_total_queries(len(test_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddfcea2-8fa4-4cd8-b4eb-f6fae82a9c58",
   "metadata": {
    "height": 472
   },
   "outputs": [],
   "source": [
    "with perf_eval:\n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\n[{i}] Question: '{question}'\")\n",
    "\n",
    "        perf_eval.start()\n",
    "\n",
    "        if cached_result := cache.check(question):\n",
    "            # Cache HIT\n",
    "            perf_eval.tick(\"cache_hit\")\n",
    "            print(\n",
    "                f\"    ‚úÖ CACHE HIT (distance: {cached_result[0]['vector_distance']:.3f})\"\n",
    "            )\n",
    "            print(f\"    üìã Cached question: {cached_result[0]['prompt'][:80]}...\")\n",
    "            print(f\"    üìã Cached response: {cached_result[0]['response'][:80]}...\")\n",
    "        else:\n",
    "            # Cache MISS - call LLM\n",
    "            perf_eval.tick(\"cache_miss\")  # Time for cache check\n",
    "            print(f\"    ‚ùå CACHE MISS\")\n",
    "            print(f\"    ü§ñ Calling LLM... \", end=\"\")\n",
    "\n",
    "            # Call LLM and track the call\n",
    "            perf_eval.start()\n",
    "            llm_response = get_llm_response(question)\n",
    "            perf_eval.tick(\"llm_call\")\n",
    "            perf_eval.record_llm_call(MODEL_NAME, question, llm_response)\n",
    "            print(f\"    üí¨ LLM response: {llm_response[:80]}...\")\n",
    "            cache.store(prompt=question, response=llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bb2a96-9142-4a55-84da-92a575d0b2fc",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "np.mean(perf_eval.durations_by_label['cache_hit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5b34bd-4b60-4a90-9201-307a73deecba",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "np.mean(perf_eval.durations_by_label['llm_call'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7069e3bf-a5ef-4a25-95ec-40b47006e727",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"><b>Note:</b> In the above experiment we measure the latency of the cache response and a mocked latency of an LLM call. The mocked LLM call is a dummy function that sleeps for a random amount of time. The randomness in the results mainly comes from the randomness we introduced to mock the LLM. The results show us what we can typically see in practice.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad2948f-1b8f-4b32-8f6f-655df47d60e2",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "cache.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0227b14e-585a-4c4d-88a5-4f2f66182f3e",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aea886-c979-401e-a852-a09be045f6d0",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d1e237-d67d-48c2-b2c9-fc7126c6730f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca220abd",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5f2c49",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73fb708-b62c-4c8b-bd20-c3845af71c7a",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
